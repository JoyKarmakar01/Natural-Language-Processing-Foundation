{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2b917c",
   "metadata": {},
   "source": [
    "#### Transformers: BERT and GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa3cc68",
   "metadata": {},
   "source": [
    "##### Definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc5ea2",
   "metadata": {},
   "source": [
    "Transformers are a type of deep learning architecture designed to handle sequential data, making them highly effective for natural language processing (NLP) tasks. They utilize self-attention mechanisms to weigh the importance of different words in a sequence, allowing them to capture long-range dependencies more effectively than traditional models like RNNs and CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0150b827",
   "metadata": {},
   "source": [
    "#### Types:\n",
    "1. BERT (Bidirectional Encoder Representations from Transformers): BERT is designed to understand the context of a word in a sentence by looking at both the words before and after it (bidirectional).\n",
    "2. GPT (Generative Pre-trained Transformer): GPT is designed for language generation tasks, using a unidirectional approach where the model predicts the next word in a sequence based on the words before it.\n",
    "Use Cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1491275b",
   "metadata": {},
   "source": [
    "#### Use Cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c04a537",
   "metadata": {},
   "source": [
    "1. Text Classification: Categorizing text into predefined categories.\n",
    "2. Named Entity Recognition (NER): Identifying and classifying proper nouns in the text.\n",
    "3. Question Answering: Building models to answer questions based on the given text.\n",
    "4. Text Generation: Creating human-like text based on given prompts.\n",
    "5. Machine Translation: Translating text from one language to another"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba54e9",
   "metadata": {},
   "source": [
    "#### Short Implementation:\n",
    "1. BERT Example for Text Classification\n",
    "2. Step 1: Install Necessary Libraries\n",
    "3. Install the transformers library from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9166486",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d305a5ef",
   "metadata": {},
   "source": [
    "#### Step 2: Import Libraries and Load BERT Model\n",
    "We'll use transformers library to load the pre-trained BERT model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19093247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sample text data\n",
    "texts = [\"I love natural language processing\", \"BERT is powerful for text classification\"]\n",
    "labels = [1, 0]  # Example labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7a335b",
   "metadata": {},
   "source": [
    "#### Step 3: Preprocess the Text Data\n",
    "Tokenize the texts and convert them to input format suitable for BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f90687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode the texts\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "input_ids = encodings['input_ids']\n",
    "attention_mask = encodings['attention_mask']\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = torch.utils.data.TensorDataset(input_ids, attention_mask, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ce497",
   "metadata": {},
   "source": [
    "#### Step 4: Define Training Arguments and Trainer\n",
    "Set up the training arguments and initialize the Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ece8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6530cfc",
   "metadata": {},
   "source": [
    "#### Step 5: Train the Model\n",
    "Train the BERT model on the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e9d01",
   "metadata": {},
   "source": [
    "#### GPT Example for Text Generation\n",
    "Step 1: Install Necessary Libraries\n",
    "Install the transformers library from Hugging Face."
   ]
  },
  {
   "cell_type": "raw",
   "id": "77b8454b",
   "metadata": {},
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe16069b",
   "metadata": {},
   "source": [
    "#### Step 2: Import Libraries and Load GPT Model\n",
    "We'll use transformers library to load the pre-trained GPT-2 model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c6f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5688eec",
   "metadata": {},
   "source": [
    "#### Step 3: Generate Text\n",
    "Use the GPT-2 model to generate text based on a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfa822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Encode the prompt and generate text\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fbbc76",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "BERT for Text Classification:\n",
    "\n",
    "Text Preprocessing: Texts are tokenized and converted into input tensors suitable for BERT.\n",
    "Model and Trainer Initialization: The BERT model and training arguments are set up.\n",
    "Training: The model is trained on the text data using the Trainer class from the transformers library.\n",
    "GPT for Text Generation:\n",
    "\n",
    "Model and Tokenizer Loading: The GPT-2 model and tokenizer are loaded.\n",
    "Text Generation: The model generates text based on a given prompt, which is then decoded and displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7960f4",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "Transformers, including models like BERT and GPT, have revolutionized NLP by providing powerful mechanisms to capture context and generate human-like text. They are highly versatile and can be fine-tuned for a wide range of NLP tasks, making them indispensable tools in modern NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a1cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
