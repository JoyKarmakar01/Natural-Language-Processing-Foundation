{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07785f10",
   "metadata": {},
   "source": [
    "#### Definition:\n",
    "The attention mechanism is a technique used in neural networks to improve the performance of models on tasks involving sequential data. It allows the model to focus on different parts of the input sequence when generating each part of the output sequence. By doing this, the model can better capture the dependencies and relevant information in the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d095e67",
   "metadata": {},
   "source": [
    "#### Types:\n",
    "1. Additive Attention: Computes a compatibility score between the decoder state and encoder states using a feed-forward network.\n",
    "2. Multiplicative (Dot-Product) Attention: Computes the compatibility score as the dot product of the decoder state and encoder states.\n",
    "3. Scaled Dot-Product Attention: Similar to dot-product attention but scales the scores by the square root of the dimension to prevent large gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d259600",
   "metadata": {},
   "source": [
    "#### Use Cases:\n",
    "1. Machine Translation: Improving translation accuracy by focusing on relevant words in the source sentence.\n",
    "2. Text Summarization: Focusing on important parts of the text to generate concise summaries.\n",
    "3. Image Captioning: Attending to different regions of an image when generating descriptive captions.\n",
    "4. Speech Recognition: Focusing on different parts of the audio signal to accurately transcribe speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd13ad",
   "metadata": {},
   "source": [
    "#### Short Implementation:\n",
    "Example: Attention Mechanism in a Seq2Seq Model for Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee5f272",
   "metadata": {},
   "source": [
    "#### Step 1: Define the Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b92db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hid_dim))\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden = [batch size, hid dim]\n",
    "        # encoder_outputs = [src len, batch size, hid dim]\n",
    "        \n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = torch.sum(self.v * energy, dim=2)\n",
    "        \n",
    "        return F.softmax(attention, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0b671b",
   "metadata": {},
   "source": [
    "#### Step 2: Integrate Attention into the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b22d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim + hid_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "        # encoder_outputs = [src len, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        a = self.attention(hidden[-1], encoder_outputs).unsqueeze(1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        \n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3e543e",
   "metadata": {},
   "source": [
    "#### Step 3: Train the Seq2Seq Model with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427aa845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize encoder, decoder, and Seq2Seq model\n",
    "INPUT_DIM = 1000\n",
    "OUTPUT_DIM = 1000\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "attn = Attention(HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop (simplified)\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3ab276",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "1. Attention Layer: Computes attention weights by comparing the current decoder state with each encoder state, allowing the model to focus on relevant parts of the input sequence.\n",
    "2. Decoder with Attention: Uses the attention weights to create a weighted sum of the encoder outputs, which is then combined with the current input and passed through the RNN to generate the next output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2e6925",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "The attention mechanism significantly enhances Seq2Seq models by allowing them to focus on relevant parts of the input sequence during decoding. This results in improved performance on tasks requiring sequence transformation, such as machine translation, text summarization, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ff7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
