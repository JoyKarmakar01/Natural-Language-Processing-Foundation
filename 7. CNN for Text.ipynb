{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8ec90a0",
   "metadata": {},
   "source": [
    "#### Definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8925f",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network (CNN) is a class of deep neural networks, originally designed for image processing, that can also be effectively applied to text data for tasks such as text classification, sentiment analysis, and more. CNNs are capable of capturing local patterns (like n-grams) in the text, making them particularly well-suited for extracting features from textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b25ec7",
   "metadata": {},
   "source": [
    "#### Types:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504a521",
   "metadata": {},
   "source": [
    "1. 1D Convolutional Neural Networks: These are typically used for text data, applying convolutions over word embeddings.\n",
    "2. Multi-Channel CNNs: These use multiple sets of filters (channels) to capture different types of features from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d34a35a",
   "metadata": {},
   "source": [
    "#### Use Cases:\n",
    "1. Text Classification: Categorizing text into predefined categories.\n",
    "2. Sentiment Analysis: Determining the sentiment expressed in a piece of text.\n",
    "3. Named Entity Recognition (NER): Identifying and classifying proper nouns in the text.\n",
    "4. Spam Detection: Identifying spam or fraudulent messages.\n",
    "5. Question Answering: Building models to answer questions based on the given text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a14756f",
   "metadata": {},
   "source": [
    "#### Step 1: Import Necessary Libraries\n",
    "We'll start by importing the necessary libraries for our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2b87ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "# Sample text data\n",
    "texts = [\"I love natural language processing\", \"CNNs are powerful for text classification\"]\n",
    "labels = [1, 0]  # Example labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324aeec6",
   "metadata": {},
   "source": [
    "#### Step 2: Preprocess the Text Data\n",
    "Tokenize the texts and convert them to sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2961ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the texts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad the sequences\n",
    "max_len = 10  # Maximum length of sequences\n",
    "data = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Get the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdcaba0",
   "metadata": {},
   "source": [
    "#### Step 3: Create the Embedding Matrix\n",
    "For simplicity, we'll use random embeddings in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe8200",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50  # Dimension of the embedding vectors\n",
    "embedding_matrix = np.random.rand(vocab_size, embedding_dim)  # Random embedding matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed2241",
   "metadata": {},
   "source": [
    "#### Step 4: Define the CNN Model\n",
    "Build and compile the CNN model for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_len,\n",
    "                    trainable=True))  # We allow training of the embedding layer\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))  # Convolutional layer\n",
    "model.add(GlobalMaxPooling1D())  # Global max pooling layer\n",
    "model.add(Dense(10, activation='relu'))  # Fully connected layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Check the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04222cd0",
   "metadata": {},
   "source": [
    "#### Step 5: Train the Model\n",
    "Train the model on the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93940830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(data, labels, epochs=10, verbose=2)\n",
    "\n",
    "# Evaluate the model (using the same data for simplicity)\n",
    "loss, accuracy = model.evaluate(data, labels, verbose=2)\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
