{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f4a1dc",
   "metadata": {},
   "source": [
    "#### Definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f829d877",
   "metadata": {},
   "source": [
    "GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm for obtaining vector representations for words. Unlike Word2Vec, which relies on local context windows to learn embeddings, GloVe leverages global statistical information of the corpus to capture semantic relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0f13e2",
   "metadata": {},
   "source": [
    "#### Types:\n",
    "GloVe embeddings can be classified based on their dimensions (e.g., 50-dimensional, 100-dimensional, 300-dimensional, etc.), which refer to the length of the vector representing each word. The higher the dimensions, the more detailed the embedding, but it also requires more computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562e57eb",
   "metadata": {},
   "source": [
    "#### Use Cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4216435e",
   "metadata": {},
   "source": [
    "1. Text Classification: Using GloVe embeddings as features for classifying text into categories.\n",
    "2. Sentiment Analysis: Analyzing sentiment by leveraging semantic similarities captured by GloVe.\n",
    "3. Named Entity Recognition (NER): Identifying proper nouns and classifying them into predefined categories.\n",
    "4. Machine Translation: Improving the quality of translations by providing semantically rich word vectors.\n",
    "5. Information Retrieval: Enhancing search algorithms by understanding semantic similarities between query terms and documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b17627",
   "metadata": {},
   "source": [
    "#### Short Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1688c1",
   "metadata": {},
   "source": [
    "#### Step 1: Download Pre-trained GloVe Vectors\n",
    "You can download pre-trained GloVe vectors from the official website. For this example, we'll use the glove.6B.100d.txt file.\n",
    "\n",
    "#### Step 2: Load GloVe Embeddings in Python\n",
    "Here, we'll load the pre-trained GloVe embeddings and use them to create a word embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a168a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "glove_file_path = 'path/to/glove.6B.100d.txt'\n",
    "embeddings_index = load_glove_embeddings(glove_file_path)\n",
    "\n",
    "# Check a word vector\n",
    "print(embeddings_index['hello'])  # Example word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded37a94",
   "metadata": {},
   "source": [
    "#### Step 3: Create an Embedding Matrix\n",
    "Next, we will create an embedding matrix that can be used in a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9561965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample text data\n",
    "texts = [\"I love natural language processing\", \"GloVe embeddings are powerful\"]\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad the sequences\n",
    "max_len = 10\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Check the embedding matrix\n",
    "print(embedding_matrix.shape)  # Should be (vocab_size, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d2cfb7",
   "metadata": {},
   "source": [
    "#### Step 4: Use the Embedding Matrix in a Keras Model\n",
    "Now, we can use the embedding matrix in a Keras model for a text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f959ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1,\n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_len,\n",
    "                    trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Dummy labels\n",
    "labels = np.array([1, 0])\n",
    "\n",
    "# Train the model\n",
    "model.fit(data, labels, epochs=10)\n",
    "\n",
    "# Check the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f90f68",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "1. Loading GloVe Embeddings: We load the pre-trained GloVe embeddings from a file and store them in a dictionary.\n",
    "2. Tokenization and Padding: The sample text data is tokenized and padded to a fixed length.\n",
    "3. Embedding Matrix: An embedding matrix is created where each row corresponds to a word vector from the GloVe embeddings.\n",
    "4. Keras Model: We define a simple Keras model using the embedding matrix. The embeddings are used as input to the model, followed by a dense layer for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcb2df0",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "GloVe embeddings provide a powerful way to capture semantic relationships between words using global statistical information from a corpus. They can be easily integrated into neural network models for various NLP tasks, making them a valuable tool in the data scientist's toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7787359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
