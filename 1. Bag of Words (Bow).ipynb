{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94aa16b9",
   "metadata": {},
   "source": [
    "#### Definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f376751",
   "metadata": {},
   "source": [
    "Bag of Words (BoW) is a simple and commonly used model in NLP to represent text data. It involves converting a text document into a fixed-size vector by counting the occurrence of each word within the document, disregarding grammar and word order but keeping multiplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99085153",
   "metadata": {},
   "source": [
    "#### Steps Involved in BoW:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad6cf7",
   "metadata": {},
   "source": [
    "Tokenization: Splitting the text into words (tokens).\n",
    "\n",
    "Vocabulary Creation: Building a list of unique words from the entire corpus.\n",
    "\n",
    "Vectorization: Counting the frequency of each word in the vocabulary for each document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d8956",
   "metadata": {},
   "source": [
    "#### Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8301f1",
   "metadata": {},
   "source": [
    "Consider the following two sentences:\n",
    "\n",
    "\"I love NLP\"\n",
    "\n",
    "\"NLP is great"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc9e327",
   "metadata": {},
   "source": [
    "#### Tokenization:\n",
    "    \n",
    "Sentence 1: [\"I\", \"love\", \"NLP\"]\n",
    "    \n",
    "Sentence 2: [\"NLP\", \"is\", \"great\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd5840e",
   "metadata": {},
   "source": [
    "#### Vocabulary Creation:\n",
    "    \n",
    "Vocabulary: [\"I\", \"love\", \"NLP\", \"is\", \"great\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e410504c",
   "metadata": {},
   "source": [
    "#### Use Cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222daabf",
   "metadata": {},
   "source": [
    "Text Classification: Classifying documents into categories like spam detection, sentiment analysis, etc.\n",
    "\n",
    "Information Retrieval: Search engines use BoW models to retrieve relevant documents based on keyword searches.\n",
    "\n",
    "Topic Modeling: Identifying topics within a large corpus of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc9ddd4",
   "metadata": {},
   "source": [
    "#### Implementation in Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337be3a7",
   "metadata": {},
   "source": [
    "We'll implement a simple Bag of Words model using CountVectorizer from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9413e06",
   "metadata": {},
   "source": [
    "#### Installation:\n",
    "Make sure you have scikit-learn installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ce1898",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\karma\\anaconda3\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\karma\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\karma\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\karma\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\karma\\anaconda3\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Collecting numpy<2.0,>=1.19.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\karma\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\karma\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\karma\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\karma\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\karma\\\\anaconda3\\\\Lib\\\\site-packages\\\\~umpy.libs\\\\libopenblas64__v0.3.23-293-gc2f4bdbb-gcc_10_3_0-2bde3a66a51006b2b53eb373ff767a3f.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\karma\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\karma\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\karma\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading numpy-1.22.4-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46ac52fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karma\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\karma\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\karma\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'love': 2, 'nlp': 3, 'is': 1, 'great': 0}\n",
      "Vectors:\n",
      " [[0 0 1 1]\n",
      " [1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\"I love NLP\", \"NLP is great\"]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the model and transform the documents into vectors\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
    "\n",
    "# Print the vectors\n",
    "print(\"Vectors:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef7a3f",
   "metadata": {},
   "source": [
    "##### Explanation:\n",
    "CountVectorizer: Converts a collection of text documents to a matrix of token counts.\n",
    "fit_transform: Fits the model and learns the vocabulary; then transforms the data into a document-term matrix.\n",
    "vocabulary_: A dictionary where keys are the words and values are their feature indices.\n",
    "toarray(): Converts the sparse matrix to a dense array."
   ]
  },
  {
   "cell_type": "raw",
   "id": "904bf25a",
   "metadata": {},
   "source": [
    "In this output, each row represents a document, and each column represents a word from the vocabulary with the count of its occurrence.\n",
    "\n",
    "Bag of Words is a foundational technique in NLP, and while it's simple, it forms the basis for more advanced text processing techniques. As you progress, you'll learn about enhancements like TF-IDF and word embeddings, which provide more nuanced representations of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d6e8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
